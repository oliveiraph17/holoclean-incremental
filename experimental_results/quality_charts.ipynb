{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# DataFrame Setup Function\n",
    "########################################\n",
    "\n",
    "def setup_dataframes(dataset_name, infer_mode, features, attribute=None):\n",
    "    if attribute:\n",
    "        log_path = dataset_name + '/monitoring_' + attribute + '_quality_log.csv'\n",
    "        train = 'train_batch'\n",
    "        infer = 'infer_batch'\n",
    "        messed_cells = 'total_repairs_grdt_correct'\n",
    "    else:\n",
    "        log_path = dataset_name + '/monitoring_quality_log.csv'\n",
    "#         log_path = 'logs-fev08/' + dataset_name + '/train_all_infer_all/monitoring_quality_log.csv'\n",
    "        train = 'skip_training_starting_batch'\n",
    "        infer = 'batch'\n",
    "        messed_cells = 'repairs_on_correct_cells'\n",
    "\n",
    "    df_full = pd.read_csv(log_path, sep=';')\n",
    "    df_full.drop_duplicates(keep='last', inplace=True)\n",
    "\n",
    "    df = df_full.loc[(df_full['infer_mode'] == infer_mode) & (df_full['features'] == features)]\n",
    "    df = df.astype({'dk_cells': 'int32', train: 'int32', infer: 'int32', 'total_errors': 'int32',\n",
    "                                'correct_repairs': 'int32', 'total_repairs_grdt': 'int32', messed_cells: 'int32',\n",
    "                                'precision': 'float64', 'recall': 'float64', 'f1': 'float64'})\n",
    "    df = df.sort_values(by=[train, infer])\n",
    "    \n",
    "    df_list = []\n",
    "    for skip_training_starting_batch in df[train].unique().tolist():\n",
    "        tmp_df = df.loc[df[train] == skip_training_starting_batch]\n",
    "        tmp_df = tmp_df.sort_values(by=[train, infer])\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Chart Plotting Function\n",
    "########################################\n",
    "\n",
    "def plot_charts(df_list, attribute=None):\n",
    "    if attribute:\n",
    "        train = 'train_batch'\n",
    "        infer = 'infer_batch'\n",
    "    else:\n",
    "        train = 'skip_training_starting_batch'\n",
    "        infer = 'batch'\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    chart_rows = 4\n",
    "    chart_columns = 5\n",
    "    fig, axes = plt.subplots(chart_rows, chart_columns, sharex=True, sharey=True,\n",
    "                             figsize=(chart_columns * 7, chart_rows * 5))\n",
    "\n",
    "    for i in range(chart_rows):\n",
    "        for j in range(chart_columns):\n",
    "            current_df = df_list[count]\n",
    "            count += 1\n",
    "            \n",
    "            current_df.loc[current_df['dk_cells'] == 0, ['precision', 'recall', 'f1']] = 1\n",
    "\n",
    "            axes[i, j].plot(infer, 'precision',\n",
    "                            data=current_df, marker='', color=palette(1), label='precision', linewidth=3)\n",
    "            axes[i, j].plot(infer, 'recall',\n",
    "                            data=current_df, marker='', color=palette(2), label='recall', linewidth=3)\n",
    "            axes[i, j].plot(infer, 'f1',\n",
    "                            data=current_df, marker='', color=palette(3), label='f1', linewidth=3)\n",
    "            axes[i, j].legend(loc=\"lower right\")\n",
    "\n",
    "            skip_training_starting_batch = current_df[train].iloc[0]\n",
    "            title_str = 'Training up to batch {}'.format(skip_training_starting_batch)\n",
    "            axes[i, j].set(title=title_str)\n",
    "\n",
    "            if j == 0:\n",
    "                axes[i, j].set(ylabel='Percentage')\n",
    "            if i == chart_rows - 1:\n",
    "                axes[i, j].set(xlabel='Batch')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_charts_acc(df_list, attribute=None, chart_type=1):\n",
    "    if attribute:\n",
    "        train = 'train_batch'\n",
    "        infer = 'infer_batch'\n",
    "        messed_cells = 'total_repairs_grdt_correct'\n",
    "    else:\n",
    "        train = 'skip_training_starting_batch'\n",
    "        infer = 'batch'\n",
    "        messed_cells = 'repairs_on_correct_cells'\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    prev_values = {'total_errors':  0, 'correct_repairs': 0, 'total_repairs_grdt': 0, messed_cells: 0}\n",
    "\n",
    "    chart_rows = 20\n",
    "    chart_columns = 5\n",
    "    fig, axes = plt.subplots(chart_rows, chart_columns,\n",
    "                             sharex=True,\n",
    "                             sharey=True,\n",
    "                             figsize=(chart_columns * 7, chart_rows * 5))\n",
    "\n",
    "    for i in range(chart_rows):\n",
    "        for j in range(chart_columns):\n",
    "            current_df = df_list[count]\n",
    "            count += 1\n",
    "            # Put the last batch in position 20th to compare everything with the \"ideal\" model (i.e., the last one).\n",
    "            # if count == 20:\n",
    "            #     current_df = df_list[99]\n",
    "\n",
    "            for metric in prev_values.keys():\n",
    "                # Sets the first entry to the accumulated value up to it.\n",
    "                current_df.loc[current_df.index[0], metric] = current_df.loc[current_df.index[0], metric] + prev_values[metric]\n",
    "                prev_values[metric] = current_df.loc[current_df.index[0], metric]\n",
    "                # Gets the accumulated values.\n",
    "                current_df[metric] = current_df[metric].cumsum()\n",
    "\n",
    "            current_df['acc_precision'] = current_df['correct_repairs'] / current_df['total_repairs_grdt']\n",
    "            current_df['acc_recall'] = current_df['correct_repairs'] / current_df['total_errors']\n",
    "            current_df['acc_f1'] = (2 * current_df['acc_precision'] * current_df['acc_recall']) \\\n",
    "                / (current_df['acc_precision'] + current_df['acc_recall'])\n",
    "\n",
    "            current_df['wrong_repairs'] = current_df['total_repairs_grdt'] - current_df['correct_repairs']\n",
    "\n",
    "            # Workaround to avoid division by zero.\n",
    "            current_df['relative_dirtiness'] = 0\n",
    "            current_df.loc[current_df['total_errors'] > 0, 'relative_dirtiness'] \\\n",
    "                = (current_df['total_errors'] - current_df['correct_repairs'] + current_df[messed_cells]) / current_df['total_errors']\n",
    "\n",
    "            if chart_type == 1:\n",
    "                # Precision, recall and F1.\n",
    "                axes[i, j].plot(infer, 'acc_precision',\n",
    "                                data=current_df, marker='', color=palette(1), label='Precision', linewidth=3)\n",
    "                axes[i, j].plot(infer, 'acc_recall',\n",
    "                                data=current_df, marker='', color=palette(2), label='Recall', linewidth=3)\n",
    "                axes[i, j].plot(infer, 'acc_f1',\n",
    "                                data=current_df, marker='', color=palette(3), label='F1', linewidth=3)\n",
    "                y_label = 'Percentage'\n",
    "                axes[i, j].set_ylim(0,1)\n",
    "                \n",
    "            elif chart_type == 2:\n",
    "                # Numbers of errors and repairs.\n",
    "                axes[i, j].plot(infer, 'correct_repairs',\n",
    "                                data=current_df, marker='', color=palette(4), label='Correct Repairs', linewidth=3)            \n",
    "                axes[i, j].plot(infer, 'total_errors',\n",
    "                                data=current_df, marker='', color=palette(5), label='Total Errors', linewidth=3)            \n",
    "                axes[i, j].plot(infer, 'total_repairs_grdt',\n",
    "                                data=current_df, marker='', color=palette(6), label='Total Repairs', linewidth=3)            \n",
    "                axes[i, j].plot(infer, 'wrong_repairs',\n",
    "                                data=current_df, marker='', color=palette(7), label='Wrong Repairs', linewidth=3)            \n",
    "                axes[i, j].plot(infer, messed_cells,\n",
    "                                data=current_df, marker='', color=palette(8), label='Cells Messed up', linewidth=3)            \n",
    "                y_label = 'Accumulated Number'            \n",
    "\n",
    "            elif chart_type == 3:\n",
    "                # Relative dirtiness (i.e., how much cleaner/dirtier the dataset becomes)\n",
    "                axes[i, j].plot(infer, 'relative_dirtiness',\n",
    "                                data=current_df, marker='', color=palette(9), label='Relative Dirtiness', linewidth=3)\n",
    "                y_label = 'Relative Dirtiness'\n",
    "\n",
    "            # To eventually skip some initial too bad results that mess up the y-scale\n",
    "            if count <= 0:\n",
    "                continue\n",
    "\n",
    "            skip_training_starting_batch = current_df[train].iloc[0]\n",
    "            title_str = 'Training up to {}%'.format(skip_training_starting_batch)\n",
    "            axes[i, j].set(title=title_str)\n",
    "            axes[i, j].legend(loc=\"lower right\")\n",
    "\n",
    "            if j == 0:\n",
    "                axes[i, j].set(ylabel=y_label)\n",
    "            axes[i, j].set(xlabel='% of the Dataset')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Charts: Setup\n",
    "########################################\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "palette = plt.get_cmap('tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Charts: Analysis\n",
    "########################################\n",
    "\n",
    "# An attribute name or None.\n",
    "attribute = None\n",
    "\n",
    "# 1st argument: 'dk' or 'all'.\n",
    "# 2nd argument: 'incremental' or 'global'.\n",
    "\n",
    "# Instant PxR chart.\n",
    "# plot_charts(setup_dataframes('nypd6', 'all', 'incremental', attribute), attribute)\n",
    "\n",
    "# Accummulated charts.\n",
    "# Chart type: 1=PxR; 2=Errors; 3=Relative Dirtiness\n",
    "chart_type = 2\n",
    "plot_charts_acc(setup_dataframes('hospital_shuffled', 'all', 'incremental', attribute), attribute, chart_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'hospital'\n",
    "df = pd.read_csv(dataset_name + '/monitoring_quality_log.csv', sep=';')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Adds infer_mode and features attributes as they're missing in the weight files\n",
    "########################################\n",
    "\n",
    "def correct_weight_file(dataset_name, attribute):\n",
    "#     weight_log_path = 'logs-fev08/' + dataset_name + '/train_all_infer_all/monitoring_' + attribute + '_weight_log'\n",
    "#     quality_log_path = 'logs-fev08/' + dataset_name + '/train_all_infer_all/monitoring_' + attribute + '_quality_log'\n",
    "    weight_log_path = dataset_name + '/monitoring_' + attribute + '_weight_log'\n",
    "    quality_log_path = dataset_name + '/monitoring_' + attribute + '_quality_log'\n",
    "    \n",
    "\n",
    "    weight_df = pd.read_csv(weight_log_path + '.csv', sep=';')\n",
    "    quality_df = pd.read_csv(quality_log_path + '.csv', sep=';')\n",
    "    \n",
    "    # Clear extra headers from the middle of the file.\n",
    "    weight_df = weight_df.loc[weight_df['w0'] != 'w0']\n",
    "    # Clear log lines not associated to training (but inferring).\n",
    "    quality_df = quality_df.loc[quality_df['train_batch'] == quality_df['infer_batch'] ]\n",
    "\n",
    "    # Align the indexes because when trying to assign a column of one DataFrame to another,\n",
    "    # pandas will try to align the indexes, and failing to do so, insert NaNs.\n",
    "    weight_df.index = quality_df.index\n",
    "\n",
    "    # Assign the columns. It works because the original row orders from the files correspond.\n",
    "    weight_df['infer_mode'] = quality_df['infer_mode']\n",
    "    weight_df['features'] = quality_df['features']\n",
    "\n",
    "    weight_df = weight_df.astype({'batch_number': 'int32'})\n",
    "    weight_df = weight_df.sort_values(by=['batch_number'])\n",
    "    \n",
    "    weight_df.to_csv(weight_log_path + '_corrected.csv', index=False, sep=';')\n",
    "    \n",
    "#correct_weight_file('soccer', 'season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import glob\n",
    "import math\n",
    "\n",
    "def get_weight_file(dataset_name, infer_mode, features, attribute):\n",
    "    weight_log_path = dataset_name + '/monitoring_' + attribute + '_weight_log_corrected'\n",
    "\n",
    "    if not os.path.isfile(weight_log_path):\n",
    "        correct_weight_file(dataset_name, attribute)\n",
    "\n",
    "    weight_df = pd.read_csv(weight_log_path + '.csv', sep=';')\n",
    "    weight_df.drop_duplicates(keep='last', inplace=True)\n",
    "    weight_df = weight_df.loc[(weight_df['infer_mode'] == infer_mode) & (weight_df['features'] == features)]\n",
    "    \n",
    "    return weight_df\n",
    "\n",
    "def plot_weights(dataset_name, infer_mode, features, attribute=None, show_errors=False):\n",
    "    \n",
    "    if attribute:\n",
    "        weight_df = get_weight_file(dataset_name, infer_mode, features, attribute)\n",
    "        ax = plt.gca()\n",
    "        for i in range(len(weight_df.columns) - 3):\n",
    "            weight_df.plot(kind='line', x='batch_number', y='w' + str(i), ax=ax)\n",
    "\n",
    "        plt.title(dataset_name + ' - ' + attribute)\n",
    "        plt.ylabel('Weight Value')\n",
    "        plt.xlabel('% of the Dataset')\n",
    "        plt.legend(loc=(1.04, 0))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        file_names = glob.glob(dataset_name + \"/*_weight_log.csv\")\n",
    "        chart_columns = 4\n",
    "        chart_rows = math.ceil(len(file_names) / chart_columns)\n",
    "        fig, axes = plt.subplots(chart_rows, chart_columns,\n",
    "#                                  sharex=True,\n",
    "#                                  sharey=True,\n",
    "                                 figsize=(chart_columns * 7, chart_rows * 5)\n",
    "                                )\n",
    "\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        for fname in file_names:\n",
    "            attr = fname.split('monitoring_')[1].split('_weight')[0]\n",
    "            \n",
    "            weight_df = get_weight_file(dataset_name, infer_mode, features, attr)\n",
    "            \n",
    "            for w in range(len(weight_df.columns) - 3):\n",
    "                try:\n",
    "                    weight_df.plot(kind='line', x='batch_number', y='w' + str(w), ax=axes[i, j], legend=False)\n",
    "#                     axes[i, j].set_xlim(1,100)\n",
    "                except:\n",
    "                    if show_errors:\n",
    "                        print('Error: {} ({})'.format(attr, w))\n",
    "                \n",
    "            axes[i, j].set(title=attr)\n",
    "\n",
    "            i += (j + 1) // chart_columns\n",
    "            j = (j + 1) % chart_columns\n",
    "                \n",
    "        fig.tight_layout()\n",
    "\n",
    "# Plot\n",
    "plot_weights('hospital_shuffled', 'dk', 'incremental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import time\n",
    "import hdbscan\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import groupby\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 1, 's': 80, 'linewidths':0}\n",
    "\n",
    "dataset_name = 'nypd6'\n",
    "base_path = '/home/dskaster/home/codigos/uwaterloo/holoclean-incremental/experimental_results/'\n",
    "\n",
    "data_corr = np.loadtxt(base_path + dataset_name + '_corr.csv', delimiter=';')\n",
    "data_dist = np.loadtxt(base_path + dataset_name + '_dist.csv', delimiter=';')\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_corr = pca.fit_transform(data_corr[:,1:])\n",
    "pca_dist = pca.fit_transform(data_dist[:,1:])\n",
    "\n",
    "# tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "# tsne_corr = tsne.fit_transform(data_corr[:,1:])\n",
    "# tsne_dist = tsne.fit_transform(data_dist[:,1:])\n",
    "\n",
    "# Clustering.\n",
    "hdbs = hdbscan.HDBSCAN(min_cluster_size=2)\n",
    "labels_corr = hdbs.fit_predict(data_corr[:,1:])\n",
    "labels_dist = hdbs.fit_predict(data_dist[:,1:])\n",
    "\n",
    "# Computes distances between correlation vectors.\n",
    "sim_corr_list = []\n",
    "for corr_vec in data_corr[:, 1:]:\n",
    "    dist_corr_vec = np.linalg.norm(corr_vec - data_corr[:, 1:], ord=2, axis=1)\n",
    "    sim_corr_list.append(dist_corr_vec)\n",
    "\n",
    "# Scales the distances to interval [0, 1].\n",
    "sim_corr = np.array(sim_corr_list)\n",
    "sim_corr = sim_corr / sim_corr.max()\n",
    "\n",
    "# Grouping based on weights' distances. Remember to also change the comparison operator below.\n",
    "# Weights' distance.\n",
    "# thresh = 0.6\n",
    "# groups_ds = data_dist[:, 1:]\n",
    "\n",
    "# Grouping based on correlation.\n",
    "thresh = 0.15\n",
    "groups_ds = sim_corr\n",
    "\n",
    "# Build the groups based on the threshold.\n",
    "groups = [set([i]) for i in range(groups_ds.shape[0])]\n",
    "for i in range(groups_ds.shape[0] - 1):\n",
    "    for j in range(i + 1, groups_ds.shape[1]):\n",
    "        \n",
    "        # Groups based on correlations.\n",
    "        # Compares both i,j and j,i because conditional entropy is not symmetric.\n",
    "        if groups_ds[i, j] <= thresh and groups_ds[j, i] <= thresh:\n",
    "            \n",
    "#         # Groups based on distance between learned weights.\n",
    "#         if groups_ds[i, j] >= thresh:\n",
    "\n",
    "            for idx, g in enumerate(groups):\n",
    "                if i in g:\n",
    "                    group_i = idx\n",
    "                if j in g:\n",
    "                    group_j = idx\n",
    "            if group_i != group_j:\n",
    "                groups[group_i].update(groups[group_j])\n",
    "                del groups[group_j]\n",
    "print(groups)\n",
    "# Eliminate duplicated groups.\n",
    "groups = list(set(frozenset(item) for item in groups))\n",
    "            \n",
    "# palette = sns.color_palette('deep', np.unique(labels_corr).max() + 1)\n",
    "# colors_corr = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels_corr]\n",
    "markers = ['o', 'v', 'X', 's', 'P', '*', '<', '^', '>', '+', 'x', 'd', 'D', 'h', '1', '2', '3', '4']\n",
    "\n",
    "chart_rows = 2\n",
    "chart_columns = 2\n",
    "fig, axes = plt.subplots(chart_rows, chart_columns, figsize=(chart_columns * 12, chart_rows * 8), subplot_kw={'projection': '3d'})\n",
    "\n",
    "axes[0, 0].set_title('PCA Correlations - HDBSCAN Clustering')\n",
    "for i in range(np.unique(labels_corr).max() + 1):\n",
    "    mask = (labels_corr == i)\n",
    "    axes[0, 0].scatter(pca_corr[mask,0], pca_corr[mask,1], pca_corr[mask,2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "\n",
    "axes[0, 1].set_title('PCA Distances - HDBSCAN Clustering')\n",
    "for i in np.unique(labels_dist):\n",
    "    mask = (labels_dist == i)\n",
    "    axes[0, 1].scatter(pca_dist[mask,0], pca_dist[mask,1], pca_dist[mask,2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "\n",
    "# Create masks for range grouping.\n",
    "mask = []\n",
    "for i in range(len(groups)):\n",
    "    mask.append([False for _ in range(len(data_corr[:, 0]))])\n",
    "    for j in range(len(data_corr[:, 0])):\n",
    "        if j in groups[i]:\n",
    "            mask[i][j] = True\n",
    "\n",
    "axes[1, 0].set_title('PCA Correlations - Range Grouping (thresh=%6.3f)' % thresh)\n",
    "for i in range(len(groups)):\n",
    "    axes[1, 0].scatter(pca_corr[mask[i],0], pca_corr[mask[i],1], pca_corr[mask[i],2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "\n",
    "axes[1, 1].set_title('PCA Distances - Range Grouping (thresh=%6.3f)' % thresh)\n",
    "for i in range(len(groups)):\n",
    "    axes[1, 1].scatter(pca_dist[mask[i],0], pca_dist[mask[i],1], pca_dist[mask[i],2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "    \n",
    "# axes[1, 0].set_title('TSNE Correlations')\n",
    "# for i in range(np.unique(labels_corr).max() + 1):\n",
    "#     mask = (labels_corr == i)\n",
    "#     axes[1, 0].scatter(tsne_corr[mask,0], tsne_corr[mask,1], tsne_corr[mask,2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "    \n",
    "# axes[1, 1].set_title('TSNE Distances')\n",
    "# for i in np.unique(labels_dist):\n",
    "#     mask = (labels_dist == i)\n",
    "#     axes[1, 1].scatter(tsne_dist[mask,0], tsne_dist[mask,1], tsne_dist[mask,2], marker=markers[i] if i>=0 else '.', **plot_kwds)\n",
    "    \n",
    "for i in range(chart_rows):\n",
    "    for j in range(chart_columns):\n",
    "        axes[i, j].get_xaxis().set_visible(False)\n",
    "        axes[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "font_size = 10\n",
    "for i, txt in enumerate(data_corr[:,0]):\n",
    "    axes[0, 0].text(pca_corr[i,0], pca_corr[i,1], pca_corr[i,2], txt, fontsize=font_size)\n",
    "    axes[0, 1].text(pca_dist[i,0], pca_dist[i,1], pca_dist[i,2], txt, fontsize=font_size)\n",
    "    axes[1, 0].text(pca_corr[i,0], pca_corr[i,1], pca_corr[i,2], txt, fontsize=font_size)\n",
    "    axes[1, 1].text(pca_dist[i,0], pca_dist[i,1], pca_dist[i,2], txt, fontsize=font_size)\n",
    "    \n",
    "    \n",
    "#     axes[1, 0].text(tsne_corr[i,0], tsne_corr[i,1], tsne_corr[i,2], txt, fontsize=font_size)\n",
    "#     axes[1, 1].text(tsne_dist[i,0], tsne_dist[i,1], tsne_dist[i,2], txt, fontsize=font_size)\n",
    "\n",
    "ds_attrs = {}\n",
    "ds_attrs['hospital_shuffled'] = ['Address1', 'City', 'Condition', 'CountyName', 'EmergencyService', 'HospitalName', \\\n",
    "                                 'HospitalOwner', 'HospitalType', 'MeasureCode', 'MeasureName', 'PhoneNumber', \\\n",
    "                                 'ProviderNumber', 'Sample', 'Score', 'State', 'Stateavg', 'ZipCode']\n",
    "\n",
    "ds_attrs['food5k_shuffled'] = ['address', 'akaname', 'city', 'dbaname', 'facilitytype', 'inspectiondate', \\\n",
    "                               'inspectionid', 'inspectiontype', 'license', 'results', 'risk', 'state', 'zip']\n",
    "\n",
    "ds_attrs['nypd6'] = ['ADDR_PCT_CD', 'BORO_NM', 'CRM_ATPT_CPTD_CD', 'JURISDICTION_CODE', 'JURIS_DESC', 'KY_CD', \\\n",
    "                     'LAW_CAT_CD', 'LOC_OF_OCCUR_DESC', 'OFNS_DESC', 'PATROL_BORO', 'PD_CD', 'PD_DESC', 'PREM_TYP_DESC']\n",
    "\n",
    "ds_attrs['soccer'] = ['name', 'surname', 'birthyear', 'birthplace', 'position', 'team', \\\n",
    "                      'city', 'stadium', 'season', 'manager']\n",
    "\n",
    "chart_columns = 3\n",
    "chart_rows = math.ceil(len(groups) / chart_columns)\n",
    "fig2, axes2 = plt.subplots(num=2, nrows=chart_rows, ncols=chart_columns, sharey=True, figsize=(10 * chart_columns, 10 * chart_rows))\n",
    "w_i = ['w' + str(i) for i in range(groups_ds.shape[0])]\n",
    "\n",
    "for idx, group in enumerate(groups):\n",
    "    row_idx = idx // chart_columns\n",
    "    col_idx = idx % chart_columns\n",
    "    for i in group:\n",
    "        weight_df = get_weight_file(dataset_name, 'all', 'incremental', ds_attrs[dataset_name][i])\n",
    "        weight_df = weight_df.loc[weight_df['batch_number'] == 100, w_i]\n",
    "        axes2[row_idx, col_idx].plot(weight_df.values.tolist()[0], label=str(i) + ') ' + ds_attrs[dataset_name][i], alpha=1/len(group))\n",
    "        \n",
    "    axes2[row_idx, col_idx].legend(loc='best')\n",
    "    axes2[row_idx, col_idx].set_title('Group: ' + ','.join(str(el) for el in group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
